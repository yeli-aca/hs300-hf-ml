# âœ… å…¨æµç¨‹ï¼šå› å­æ„é€  â†’ æ ‡å‡†åŒ– â†’ Lasso / XGBoost / PLSâ†’DNNï¼ˆIPCAæ›¿ä»£ï¼‰â†’ SHAP â†’ï¼ˆå¯é€‰å¯¼å‡ºï¼‰

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.linear_model import LassoCV
from sklearn.cross_decomposition import PLSRegression  # ç›‘ç£å¼é™ç»´ï¼ŒIPCA çš„å®ç”¨æ›¿ä»£
from sklearn.decomposition import PCA

import xgboost as xgb
import shap

# ---- (å¯é€‰) æ·±åº¦å­¦ä¹  ----
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
import tensorflow as tf

# ============== åŸºæœ¬è®¾ç½® ==============
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)
tf.random.set_seed(RANDOM_STATE)

desktop_path = os.path.expanduser("~/Desktop")
stock_file = os.path.join(desktop_path, "æ²ªæ·±300æˆåˆ†æ•°æ®.xlsx")
index_file = os.path.join(desktop_path, "æ²ªæ·±300æŒ‡æ•°.xlsx")

# ============== è¯»å–æ•°æ® ==============
stock_df = pd.read_excel(stock_file, sheet_name='æ—¥åº¦')
index_df = pd.read_excel(index_file, sheet_name='Sheet1')

# æ—¥æœŸæ ¼å¼
stock_df['äº¤æ˜“æ—¥æœŸ'] = pd.to_datetime(stock_df['äº¤æ˜“æ—¥æœŸ'], errors='coerce')
index_df = index_df.rename(columns={index_df.columns[0]: 'æ—¥æœŸ'})
index_df['æ—¥æœŸ'] = pd.to_datetime(index_df['æ—¥æœŸ'], errors='coerce')

# æŒ‡æ•°å­—æ®µé‡å‘½å + æ•°å€¼åŒ–
index_df.columns = ['æ—¥æœŸ', 'å¼€ç›˜ä»·', 'æœ€é«˜ä»·', 'æœ€ä½ä»·', 'æ”¶ç›˜ä»·', 'æˆäº¤é¢', 'æˆäº¤é‡']
num_cols = ['å¼€ç›˜ä»·', 'æœ€é«˜ä»·', 'æœ€ä½ä»·', 'æ”¶ç›˜ä»·', 'æˆäº¤é¢', 'æˆäº¤é‡']
index_df[num_cols] = index_df[num_cols].apply(pd.to_numeric, errors='coerce')

# ============== æ”¶ç›Šç‡æ„é€  ==============
stock_df = stock_df.sort_values(['è¯åˆ¸ä»£ç ', 'äº¤æ˜“æ—¥æœŸ'])
stock_df['å‰æ—¥æ”¶ç›˜ä»·'] = stock_df.groupby('è¯åˆ¸ä»£ç ')['æ—¥æ”¶ç›˜ä»·'].shift(1)
stock_df['ä¸ªè‚¡æ—¥æ”¶ç›Šç‡'] = np.log(stock_df['æ—¥æ”¶ç›˜ä»·'] / stock_df['å‰æ—¥æ”¶ç›˜ä»·'])

index_df = index_df.sort_values('æ—¥æœŸ')
index_df['å‰æ—¥æ”¶ç›˜ä»·'] = index_df['æ”¶ç›˜ä»·'].shift(1)
index_df['æŒ‡æ•°æ—¥æ”¶ç›Šç‡'] = np.log(index_df['æ”¶ç›˜ä»·'] / index_df['å‰æ—¥æ”¶ç›˜ä»·'])

# åˆå¹¶ & è¶…é¢æ”¶ç›Š
merged_df = pd.merge(stock_df, index_df[['æ—¥æœŸ', 'æŒ‡æ•°æ—¥æ”¶ç›Šç‡']],
                     left_on='äº¤æ˜“æ—¥æœŸ', right_on='æ—¥æœŸ', how='left')
merged_df['è¶…é¢æ”¶ç›Šç‡'] = merged_df['ä¸ªè‚¡æ—¥æ”¶ç›Šç‡'] - merged_df['æŒ‡æ•°æ—¥æ”¶ç›Šç‡']

# ============== å› å­æ„é€ ï¼ˆ22ä¸ªï¼Œå¯å†æ‰©å±•ï¼‰ ==============
df = merged_df.copy().sort_values(['è¯åˆ¸ä»£ç ', 'äº¤æ˜“æ—¥æœŸ'])

# åŠ¨é‡ç±»
df['ret_1d'] = np.log(df['æ—¥æ”¶ç›˜ä»·'] / df.groupby('è¯åˆ¸ä»£ç ')['æ—¥æ”¶ç›˜ä»·'].shift(1))
df['ret_5d'] = np.log(df['æ—¥æ”¶ç›˜ä»·'] / df.groupby('è¯åˆ¸ä»£ç ')['æ—¥æ”¶ç›˜ä»·'].shift(5))
df['ret_10d'] = np.log(df['æ—¥æ”¶ç›˜ä»·'] / df.groupby('è¯åˆ¸ä»£ç ')['æ—¥æ”¶ç›˜ä»·'].shift(10))
df['reversal_1d'] = -1 * df.groupby('è¯åˆ¸ä»£ç ')['ret_1d'].shift(1)
df['momentum_ratio'] = df['ret_5d'] / df['ret_10d'].replace(0, np.nan)

# æ³¢åŠ¨ç±»
df['vol_5d'] = df.groupby('è¯åˆ¸ä»£ç ')['ret_1d'].rolling(5).std().reset_index(level=0, drop=True)
df['vol_10d'] = df.groupby('è¯åˆ¸ä»£ç ')['ret_1d'].rolling(10).std().reset_index(level=0, drop=True)
df['high_low_diff'] = df['æ—¥æœ€é«˜ä»·'] - df['æ—¥æœ€ä½ä»·']
df['real_body'] = (df['æ—¥æ”¶ç›˜ä»·'] - df['æ—¥å¼€ç›˜ä»·']).abs()
df['return_vol_ratio'] = df['ret_5d'] / df['vol_5d']

# æˆäº¤ç±»
df['volume_ratio'] = df.groupby('è¯åˆ¸ä»£ç ')['æ—¥ä¸ªè‚¡äº¤æ˜“è‚¡æ•°'].apply(lambda x: x / x.rolling(5).mean())
df['volume_std_5d'] = df.groupby('è¯åˆ¸ä»£ç ')['æ—¥ä¸ªè‚¡äº¤æ˜“è‚¡æ•°'].rolling(5).std().reset_index(level=0, drop=True)
df['amount_per_share'] = df['æ—¥ä¸ªè‚¡äº¤æ˜“é‡‘é¢'] / df['æ—¥ä¸ªè‚¡äº¤æ˜“è‚¡æ•°'].replace(0, np.nan)
df['volume_change_1d'] = df.groupby('è¯åˆ¸ä»£ç ')['æ—¥ä¸ªè‚¡äº¤æ˜“è‚¡æ•°'].apply(lambda x: x / x.shift(1))

# å‡çº¿ç±»
df['ma_5'] = df.groupby('è¯åˆ¸ä»£ç ')['æ—¥æ”¶ç›˜ä»·'].rolling(5).mean().reset_index(level=0, drop=True)
df['ma_10'] = df.groupby('è¯åˆ¸ä»£ç ')['æ—¥æ”¶ç›˜ä»·'].rolling(10).mean().reset_index(level=0, drop=True)
df['ma_diff'] = df['æ—¥æ”¶ç›˜ä»·'] - df['ma_5']
df['ma_bias_10'] = (df['æ—¥æ”¶ç›˜ä»·'] - df['ma_10']) / df['ma_10']

# ç»“æ„ç±»
df['open_close_diff'] = (df['æ—¥æ”¶ç›˜ä»·'] - df['æ—¥å¼€ç›˜ä»·']) / df['æ—¥å¼€ç›˜ä»·']
df['upper_shadow'] = df['æ—¥æœ€é«˜ä»·'] - df[['æ—¥å¼€ç›˜ä»·', 'æ—¥æ”¶ç›˜ä»·']].max(axis=1)
df['lower_shadow'] = df[['æ—¥å¼€ç›˜ä»·', 'æ—¥æ”¶ç›˜ä»·']].min(axis=1) - df['æ—¥æœ€ä½ä»·']
df['body_range_ratio'] = df['real_body'] / df['high_low_diff'].replace(0, np.nan)

factor_cols = [
    'ret_1d','ret_5d','ret_10d','reversal_1d','momentum_ratio',
    'vol_5d','vol_10d','high_low_diff','real_body','return_vol_ratio',
    'volume_ratio','volume_std_5d','amount_per_share','volume_change_1d',
    'ma_5','ma_10','ma_diff','ma_bias_10',
    'open_close_diff','upper_shadow','lower_shadow','body_range_ratio'
]

# ============== æ¨ªæˆªé¢ Z-scoreï¼ˆå«é›¶æ–¹å·®ä¿æŠ¤ï¼‰ ==============
def cross_section_zscore(g):
    mean = g.mean()
    std = g.std(ddof=0)
    if std == 0 or np.isnan(std):
        return (g - mean) * 0
    return (g - mean) / std

for col in factor_cols:
    df[col + '_z'] = df.groupby('äº¤æ˜“æ—¥æœŸ')[col].transform(cross_section_zscore)

factor_cols_z = [c + '_z' for c in factor_cols]

# ============== å»ºæ¨¡æ•°æ®é›†ï¼ˆç»Ÿä¸€ï¼‰ ==============
X = df[factor_cols_z]
y = df['è¶…é¢æ”¶ç›Šç‡']
data = pd.concat([X, y], axis=1).dropna()
X_clean = data[factor_cols_z]
y_clean = data['è¶…é¢æ”¶ç›Šç‡']

X_train, X_test, y_train, y_test = train_test_split(
    X_clean, y_clean, test_size=0.3, random_state=RANDOM_STATE
)

def report_result(name, y_true, y_pred):
    r2 = r2_score(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    print(f"âœ… {name} RÂ²: {r2:.6f}")
    print(f"ğŸ“‰ {name} MSE: {mse:.6e}")
    return r2, mse

# ============== Lasso ==============
lasso = LassoCV(cv=5, random_state=RANDOM_STATE, n_jobs=None)
lasso.fit(X_train, y_train)
y_pred_lasso = lasso.predict(X_test)
report_result("Lasso", y_test, y_pred_lasso)

coef_series = pd.Series(lasso.coef_, index=factor_cols_z)
print("\nğŸ“Š Lasso éé›¶å› å­ï¼ˆæŒ‰ç»å¯¹å€¼æ’åºï¼‰:")
print(coef_series[coef_series != 0].sort_values(key=np.abs, ascending=False))

# ============== XGBoost ==============
xgb_model = xgb.XGBRegressor(
    n_estimators=300,
    max_depth=4,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=RANDOM_STATE,
    n_jobs=-1
)
xgb_model.fit(X_train, y_train)
y_pred_xgb = xgb_model.predict(X_test)
report_result("XGBoost", y_test, y_pred_xgb)

# ç‰¹å¾é‡è¦æ€§å›¾
imp = pd.Series(xgb_model.feature_importances_, index=factor_cols_z).sort_values()
plt.figure(figsize=(8,5))
imp.plot(kind='barh')
plt.title("XGBoost Feature Importance")
plt.xlabel("Importance")
plt.tight_layout()
plt.show()

# SHAPï¼ˆå¯¹æ ‘æ¨¡å‹é€‚é…è‰¯å¥½ï¼‰
explainer = shap.Explainer(xgb_model)
shap_values = explainer(X_clean)
shap.summary_plot(shap_values, X_clean, show=True)

# ============== â€œIPCA æ›¿ä»£â€é™ç»´ â†’ DNN ==============
# æ–¹æ¡ˆAï¼ˆé»˜è®¤ï¼‰ï¼šPLSRegressionï¼ˆç›‘ç£å¼å› å­æç‚¼ï¼Œè¿‘ä¼¼ IPCA çš„â€œç”¨ y æŒ‡å¯¼æ‰¾å› å­â€æ€æƒ³ï¼‰
USE_PLS = True
N_COMPONENTS = 5  # å¯è°ƒï¼Œå»ºè®® 3~8

if USE_PLS:
    reducer = PLSRegression(n_components=N_COMPONENTS, scale=False)
    reducer.fit(X_train, y_train)
    Z_train = reducer.transform(X_train)  # ä½ç»´æ–°å› å­
    Z_test  = reducer.transform(X_test)
    reducer_name = f"PLS({N_COMPONENTS})"
else:
    # æ–¹æ¡ˆBï¼šæ— ç›‘ç£ PCAï¼ˆå¦‚éœ€å¯¹æ¯”ï¼ŒæŠŠ USE_PLS=Falseï¼‰
    pca = PCA(n_components=N_COMPONENTS, random_state=RANDOM_STATE)
    pca.fit(X_train)
    Z_train = pca.transform(X_train)
    Z_test  = pca.transform(X_test)
    reducer_name = f"PCA({N_COMPONENTS})"
    print("PCA explained variance ratio (sum):", pca.explained_variance_ratio_.sum())

# DNN ä½¿ç”¨é™ç»´åçš„è¾“å…¥ï¼ˆæ›´ç¨³ã€æ›´æ˜“å­¦ï¼‰
dnn = Sequential([
    Dense(64, input_dim=Z_train.shape[1], activation='relu'),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(1)
])
dnn.compile(optimizer=Adam(learning_rate=0.001), loss='mse')
dnn.fit(Z_train, y_train, epochs=60, batch_size=128, validation_split=0.2, verbose=0)

y_pred_dnn = dnn.predict(Z_test).flatten()
report_result(f"DNN + {reducer_name}", y_test, y_pred_dnn)

# ============== ï¼ˆå¯é€‰ï¼‰å¯¼å‡ºæ•°æ® ==============
# output_cols = ['è¯åˆ¸ä»£ç ','äº¤æ˜“æ—¥æœŸ','æ—¥æ”¶ç›˜ä»·','ä¸ªè‚¡æ—¥æ”¶ç›Šç‡','æŒ‡æ•°æ—¥æ”¶ç›Šç‡','è¶…é¢æ”¶ç›Šç‡'] + factor_cols + factor_cols_z
# output_df = df[output_cols].dropna(subset=['è¶…é¢æ”¶ç›Šç‡'])
# output_df.to_excel(os.path.join(desktop_path, "å› å­æ•°æ®_æ ‡å‡†åŒ–åŠåŸå§‹.xlsx"), index=False)
