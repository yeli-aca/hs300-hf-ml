# -*- coding: utf-8 -*-
# æ—¥åº¦ï¼šå› å­æ„é€  â†’ Z-score â†’ Lasso / XGBoost / DNN â†’ SHAP
# + è‡ªåŠ¨è¾“å‡ºåŸºç¡€å›¾è¡¨ï¼ˆFig1/2/5ï¼‰ä¸è¡¨æ ¼ï¼ˆTable_D2ï¼‰

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import EarlyStopping

from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.linear_model import LassoCV

import xgboost as xgb
import shap
import warnings
warnings.filterwarnings("ignore")

# ================== åŸºæœ¬è®¾ç½® ==================
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)
tf.random.set_seed(RANDOM_STATE)

desktop_path = os.path.expanduser("~/Desktop")
stock_file = os.path.join(desktop_path, "æ²ªæ·±300æˆåˆ†æ•°æ®.xlsx")  # sheet: æ—¥åº¦
index_file = os.path.join(desktop_path, "æ²ªæ·±300æŒ‡æ•°.xlsx")     # sheet: Sheet1
out_dir = desktop_path  # æ‰€æœ‰è¾“å‡ºç»Ÿä¸€åˆ°æ¡Œé¢ï¼›å¦‚éœ€å•ç‹¬æ–‡ä»¶å¤¹å¯æ”¹ä¸º os.path.join(desktop_path, "daily_output")

# ================== è¯»å–æ•°æ® ==================
stock_df = pd.read_excel(stock_file, sheet_name="æ—¥åº¦")
index_df = pd.read_excel(index_file, sheet_name="Sheet1")

# æ—¥æœŸä¸å­—æ®µ
stock_df["äº¤æ˜“æ—¥æœŸ"] = pd.to_datetime(stock_df["äº¤æ˜“æ—¥æœŸ"], errors="coerce")
index_df = index_df.rename(columns={index_df.columns[0]: "æ—¥æœŸ"})
index_df["æ—¥æœŸ"] = pd.to_datetime(index_df["æ—¥æœŸ"], errors="coerce")
index_df.columns = ["æ—¥æœŸ", "å¼€ç›˜ä»·", "æœ€é«˜ä»·", "æœ€ä½ä»·", "æ”¶ç›˜ä»·", "æˆäº¤é¢", "æˆäº¤é‡"]
for c in ["å¼€ç›˜ä»·","æœ€é«˜ä»·","æœ€ä½ä»·","æ”¶ç›˜ä»·","æˆäº¤é¢","æˆäº¤é‡"]:
    index_df[c] = pd.to_numeric(index_df[c], errors="coerce")

# ================== æ”¶ç›Šç‡æ„é€  ==================
stock_df = stock_df.sort_values(["è¯åˆ¸ä»£ç ","äº¤æ˜“æ—¥æœŸ"])
stock_df["å‰æ—¥æ”¶ç›˜ä»·"] = stock_df.groupby("è¯åˆ¸ä»£ç ")["æ—¥æ”¶ç›˜ä»·"].shift(1)
stock_df["ä¸ªè‚¡æ—¥æ”¶ç›Šç‡"] = np.log(stock_df["æ—¥æ”¶ç›˜ä»·"] / stock_df["å‰æ—¥æ”¶ç›˜ä»·"])

index_df = index_df.sort_values("æ—¥æœŸ")
index_df["å‰æ—¥æ”¶ç›˜ä»·"] = index_df["æ”¶ç›˜ä»·"].shift(1)
index_df["æŒ‡æ•°æ—¥æ”¶ç›Šç‡"] = np.log(index_df["æ”¶ç›˜ä»·"] / index_df["å‰æ—¥æ”¶ç›˜ä»·"])

merged_df = pd.merge(
    stock_df, index_df[["æ—¥æœŸ","æŒ‡æ•°æ—¥æ”¶ç›Šç‡"]],
    left_on="äº¤æ˜“æ—¥æœŸ", right_on="æ—¥æœŸ", how="left"
)
merged_df["è¶…é¢æ”¶ç›Šç‡"] = merged_df["ä¸ªè‚¡æ—¥æ”¶ç›Šç‡"] - merged_df["æŒ‡æ•°æ—¥æ”¶ç›Šç‡"]

# ================== åŸºç¡€å›¾è¡¨ï¼ˆè‡ªåŠ¨ä¿å­˜ï¼‰ ==================
# Fig1ï¼šè¶…é¢æ”¶ç›Šç‡åˆ†å¸ƒ
plt.figure(figsize=(8,5))
sns.histplot(merged_df["è¶…é¢æ”¶ç›Šç‡"].dropna(), bins=100, kde=True)
plt.title("Distribution of Excess Returns")
plt.xlabel("Excess Return"); plt.ylabel("Frequency")
plt.tight_layout()
plt.savefig(os.path.join(out_dir, "Fig1_Excess_Return_Distribution.png"), dpi=300)
plt.close()

# Fig2ï¼šæŒ‡æ•°æ»šåŠ¨æ³¢åŠ¨ç‡ï¼ˆ30æ—¥ï¼‰
idx_tmp = index_df.copy().sort_values("æ—¥æœŸ")
idx_tmp["æŒ‡æ•°æ—¥æ”¶ç›Šç‡"] = np.log(idx_tmp["æ”¶ç›˜ä»·"] / idx_tmp["æ”¶ç›˜ä»·"].shift(1))
idx_tmp["Rolling_Vol_30"] = idx_tmp["æŒ‡æ•°æ—¥æ”¶ç›Šç‡"].rolling(30).std()
plt.figure(figsize=(10,5))
plt.plot(idx_tmp["æ—¥æœŸ"], idx_tmp["Rolling_Vol_30"])
plt.title("HS300 Rolling Volatility (30D)")
plt.xlabel("Date"); plt.ylabel("Volatility")
plt.tight_layout()
plt.savefig(os.path.join(out_dir, "Fig2_Market_Volatility.png"), dpi=300)
plt.close()

# ================== å› å­æ„é€ ï¼ˆ22ä¸ªï¼‰ ==================
df = merged_df.copy().sort_values(["è¯åˆ¸ä»£ç ","äº¤æ˜“æ—¥æœŸ"])

# åŠ¨é‡ç±»
df["ret_1d"] = np.log(df["æ—¥æ”¶ç›˜ä»·"] / df.groupby("è¯åˆ¸ä»£ç ")["æ—¥æ”¶ç›˜ä»·"].shift(1))
df["ret_5d"] = np.log(df["æ—¥æ”¶ç›˜ä»·"] / df.groupby("è¯åˆ¸ä»£ç ")["æ—¥æ”¶ç›˜ä»·"].shift(5))
df["ret_10d"] = np.log(df["æ—¥æ”¶ç›˜ä»·"] / df.groupby("è¯åˆ¸ä»£ç ")["æ—¥æ”¶ç›˜ä»·"].shift(10))
df["reversal_1d"] = -1 * df.groupby("è¯åˆ¸ä»£ç ")["ret_1d"].shift(1)
df["momentum_ratio"] = df["ret_5d"] / df["ret_10d"].replace(0, np.nan)

# æ³¢åŠ¨ç±»
df["vol_5d"] = df.groupby("è¯åˆ¸ä»£ç ")["ret_1d"].rolling(5).std().reset_index(level=0, drop=True)
df["vol_10d"] = df.groupby("è¯åˆ¸ä»£ç ")["ret_1d"].rolling(10).std().reset_index(level=0, drop=True)
df["high_low_diff"] = df["æ—¥æœ€é«˜ä»·"] - df["æ—¥æœ€ä½ä»·"]
df["real_body"] = (df["æ—¥æ”¶ç›˜ä»·"] - df["æ—¥å¼€ç›˜ä»·"]).abs()
df["return_vol_ratio"] = df["ret_5d"] / df["vol_5d"]

# æˆäº¤ç±»
df["volume_ratio"] = df.groupby("è¯åˆ¸ä»£ç ")["æ—¥ä¸ªè‚¡äº¤æ˜“è‚¡æ•°"].apply(lambda x: x / x.rolling(5).mean())
df["volume_std_5d"] = df.groupby("è¯åˆ¸ä»£ç ")["æ—¥ä¸ªè‚¡äº¤æ˜“è‚¡æ•°"].rolling(5).std().reset_index(level=0, drop=True)
df["amount_per_share"] = df["æ—¥ä¸ªè‚¡äº¤æ˜“é‡‘é¢"] / df["æ—¥ä¸ªè‚¡äº¤æ˜“è‚¡æ•°"].replace(0, np.nan)
df["volume_change_1d"] = df.groupby("è¯åˆ¸ä»£ç ")["æ—¥ä¸ªè‚¡äº¤æ˜“è‚¡æ•°"].apply(lambda x: x / x.shift(1))

# å‡çº¿ç±»
df["ma_5"] = df.groupby("è¯åˆ¸ä»£ç ")["æ—¥æ”¶ç›˜ä»·"].rolling(5).mean().reset_index(level=0, drop=True)
df["ma_10"] = df.groupby("è¯åˆ¸ä»£ç ")["æ—¥æ”¶ç›˜ä»·"].rolling(10).mean().reset_index(level=0, drop=True)
df["ma_diff"] = df["æ—¥æ”¶ç›˜ä»·"] - df["ma_5"]
df["ma_bias_10"] = (df["æ—¥æ”¶ç›˜ä»·"] - df["ma_10"]) / df["ma_10"]

# å½¢æ€ç±»
df["open_close_diff"] = (df["æ—¥æ”¶ç›˜ä»·"] - df["æ—¥å¼€ç›˜ä»·"]) / df["æ—¥å¼€ç›˜ä»·"]
df["upper_shadow"] = df["æ—¥æœ€é«˜ä»·"] - df[["æ—¥å¼€ç›˜ä»·","æ—¥æ”¶ç›˜ä»·"]].max(axis=1)
df["lower_shadow"] = df[["æ—¥å¼€ç›˜ä»·","æ—¥æ”¶ç›˜ä»·"]].min(axis=1) - df["æ—¥æœ€ä½ä»·"]
df["body_range_ratio"] = df["real_body"] / df["high_low_diff"].replace(0, np.nan)

factor_cols = [
    "ret_1d","ret_5d","ret_10d","reversal_1d","momentum_ratio",
    "vol_5d","vol_10d","high_low_diff","real_body","return_vol_ratio",
    "volume_ratio","volume_std_5d","amount_per_share","volume_change_1d",
    "ma_5","ma_10","ma_diff","ma_bias_10",
    "open_close_diff","upper_shadow","lower_shadow","body_range_ratio"
]

# ================== æ¨ªæˆªé¢ Z-score ==================
def cross_section_zscore(g: pd.Series):
    mean = g.mean(); std = g.std(ddof=0)
    if std == 0 or np.isnan(std): return (g - mean) * 0
    return (g - mean) / std

for c in factor_cols:
    df[c + "_z"] = df.groupby("äº¤æ˜“æ—¥æœŸ")[c].transform(cross_section_zscore)

factor_cols_z = [c + "_z" for c in factor_cols]

# Fig5ï¼šå› å­ç›¸å…³æ€§çƒ­åŠ›å›¾ï¼ˆæŠ½æ ·ä»¥é˜²è¿‡å¤§ï¼‰
corr_sample = df[factor_cols_z].dropna()
if len(corr_sample) > 50000:
    corr_sample = corr_sample.sample(50000, random_state=RANDOM_STATE)
corr = corr_sample.corr()
plt.figure(figsize=(8,6))
sns.heatmap(corr, cmap="coolwarm", center=0)
plt.title("Correlation Heatmap (Standardized Factors)")
plt.tight_layout()
plt.savefig(os.path.join(out_dir, "Fig5_Factor_Correlation_Heatmap.png"), dpi=300)
plt.close()

# ================== è®­ç»ƒ/æµ‹è¯•åˆ’åˆ† ==================
data = pd.concat([df[factor_cols_z], df["è¶…é¢æ”¶ç›Šç‡"]], axis=1).dropna()
X = data[factor_cols_z].astype(float)
y = data["è¶…é¢æ”¶ç›Šç‡"].astype(float)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.30, random_state=RANDOM_STATE
)

def report_result(name, y_true, y_pred):
    r2 = r2_score(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    print(f"âœ… {name} RÂ²: {r2:.6f}")
    print(f"ğŸ“‰ {name} MSE: {mse:.6e}")
    return r2, mse

# ================== Lasso ==================
lasso = LassoCV(cv=5, random_state=RANDOM_STATE)
lasso.fit(X_train, y_train)
y_pred_lasso = lasso.predict(X_test)
r2_lasso, mse_lasso = report_result("Lasso", y_test, y_pred_lasso)

# å¯¼å‡ºã€è¡¨ D2ã€‘ï¼šLasso éé›¶ç³»æ•°
coef_series = pd.Series(lasso.coef_, index=factor_cols_z)
nz = coef_series[coef_series != 0].sort_values(key=np.abs, ascending=False)
nz.to_csv(os.path.join(out_dir, "Table_D2_Lasso_Nonzero_Coeff.csv"), encoding="utf-8")
print("\nğŸ“Š å·²å¯¼å‡ºï¼šTable_D2_Lasso_Nonzero_Coeff.csv")

# ================== XGBoost ==================
xgb_model = xgb.XGBRegressor(
    n_estimators=300, max_depth=4, learning_rate=0.05,
    subsample=0.8, colsample_bytree=0.8,
    random_state=RANDOM_STATE, n_jobs=-1
)
xgb_model.fit(X_train, y_train)
y_pred_xgb = xgb_model.predict(X_test)
r2_xgb, mse_xgb = report_result("XGBoost", y_test, y_pred_xgb)

# ç‰¹å¾é‡è¦æ€§å›¾ï¼ˆä¿å­˜ï¼‰
imp = pd.Series(xgb_model.feature_importances_, index=factor_cols_z).sort_values()
plt.figure(figsize=(8,6))
imp.plot(kind="barh")
plt.title("XGBoost Feature Importance (Daily)")
plt.xlabel("Importance")
plt.tight_layout()
plt.savefig(os.path.join(out_dir, "Fig_XGB_Feature_Importance_Daily.png"), dpi=300)
plt.close()

# ================== SHAPï¼ˆæ ‘æ¨¡å‹ï¼‰ ==================
try:
    explainer = shap.Explainer(xgb_model)
    shap_values = explainer(X)
    plt.figure()
    shap.summary_plot(shap_values, X, show=False)
    plt.tight_layout()
    plt.savefig(os.path.join(out_dir, "Fig_SHAP_Summary_Daily.png"), dpi=300, bbox_inches="tight")
    plt.close()
    print("ğŸ–¼ å·²ä¿å­˜ï¼šFig_SHAP_Summary_Daily.png")
except Exception as e:
    print(f"[SHAP] è·³è¿‡ï¼š{e}")

# ================== DNNï¼ˆè°ƒä¼˜ç‰ˆï¼‰ ==================
dnn = Sequential([
    Dense(128, input_dim=X_train.shape[1], activation="relu", kernel_regularizer=l2(1e-4)),
    BatchNormalization(), Dropout(0.30),
    Dense(64, activation="relu", kernel_regularizer=l2(1e-4)),
    BatchNormalization(), Dropout(0.30),
    Dense(32, activation="relu", kernel_regularizer=l2(1e-4)),
    Dropout(0.20),
    Dense(1)
])
dnn.compile(optimizer=Adam(learning_rate=5e-4), loss="mse")
early_stop = EarlyStopping(monitor="val_loss", patience=10, restore_best_weights=True)

hist = dnn.fit(
    X_train, y_train,
    epochs=200, batch_size=128, validation_split=0.2,
    callbacks=[early_stop], verbose=1
)

y_pred_dnn = dnn.predict(X_test).flatten()
r2_dnn, mse_dnn = report_result("Tuned DNN", y_test, y_pred_dnn)

# å­¦ä¹ æ›²çº¿
plt.figure(figsize=(8,4))
plt.plot(hist.history.get("loss",[]), label="train")
if "val_loss" in hist.history: plt.plot(hist.history["val_loss"], label="val")
plt.title("DNN Learning Curve (MSE)")
plt.xlabel("Epoch"); plt.ylabel("Loss"); plt.legend()
plt.tight_layout()
plt.savefig(os.path.join(out_dir, "Fig_DNN_Learning_Curve_Daily.png"), dpi=300)
plt.close()

# ================== æŒ‡æ ‡æ±‡æ€»ï¼ˆä¾¿äºè®ºæ–‡è¡¨æ ¼ï¼‰ ==================
summary = pd.DataFrame({
    "model": ["lasso","xgboost","dnn"],
    "R2": [r2_lasso, r2_xgb, r2_dnn],
    "MSE": [mse_lasso, mse_xgb, mse_dnn]
})
summary.to_csv(os.path.join(out_dir, "Table_Daily_Model_Metrics.csv"), index=False, encoding="utf-8")
print("âœ… å·²ä¿å­˜ï¼šTable_Daily_Model_Metrics.csv")

print("\n[INFO] æ‰€æœ‰å›¾ç‰‡ä¸è¡¨æ ¼å·²è¾“å‡ºåˆ°ï¼š", out_dir)
