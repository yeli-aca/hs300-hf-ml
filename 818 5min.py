# -*- coding: utf-8 -*-
"""
HS300 5-min å…¨æµç¨‹è„šæœ¬
- æ•°æ®é¢„å¤„ç†ï¼ˆæ‰¹é‡ CSV åˆå¹¶ + æŒ‡æ•°å­—æ® + åŸºæœ¬é¢/è¡Œä¸šå¯é€‰ï¼‰
- ç‰¹å¾æ„é€ ï¼ˆOHLCV æŠ€æœ¯ç‰¹å¾ï¼Œæ¨ªæˆªé¢ Z-scoreï¼‰
- ç›®æ ‡å˜é‡ï¼šä¸‹ä¸€æœŸ(5min)ä¸ªè‚¡è¶…é¢æ”¶ç›Š (r_i,t+1 - r_mkt,t+1)
- æ¨¡å‹ï¼šLassoCV / XGBoost / DNN(ä¼˜åŒ–ç‰ˆ)
- è¯„ä¼°ï¼šR2ã€MSEï¼›å›¾è¡¨ï¼šåˆ†å¸ƒå›¾ã€æ³¢åŠ¨ç‡ã€ç‰¹å¾é‡è¦æ€§ã€å­¦ä¹ æ›²çº¿ã€(å¯é€‰)SHAP
"""

import os
import gc
import glob
import math
import json
import warnings
from datetime import datetime

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# å¯é€‰å¢å¼ºï¼ˆæ²¡æœ‰ä¹Ÿä¸å½±å“ï¼‰
try:
    import seaborn as sns
    SEABORN = True
    sns.set(style="whitegrid")
except Exception:
    SEABORN = False

warnings.filterwarnings("ignore")

# ---------------------------
# 0. è¿è¡Œå‚æ•°ï¼ˆå·²ä¿®æ­£æ ¼å¼ï¼‰
# ---------------------------
CONFIG = {
    # è‚¡ç¥¨CSVæ–‡ä»¶å¤¹è·¯å¾„
    "STOCK_DIR": "/Users/mengyao/Desktop/5åˆ†é’Ÿ",

    # æ²ªæ·±300æŒ‡æ•°5minè¡Œæƒ…ï¼ˆè¯·ç¡®è®¤å·²æ”¹å­—æ®µåï¼‰
    "INDEX_FILE": "/Users/mengyao/Desktop/000300_æ²ªæ·±300_5min.csv",

    # åŸºæœ¬é¢ï¼ˆè¯·ç¡®è®¤å·²æ”¹å­—æ®µåï¼‰
    "FUND_FILE": "/Users/mengyao/Desktop/å­£åº¦ (1).xlsx",

    # è¾“å‡ºç›®å½•
    "OUT_DIR": os.path.expanduser("~/Desktop/hs300_5min_output"),

    # æŠ½æ ·æ¯”ä¾‹ï¼ˆNoneä¸ºä¸æŠ½æ ·ï¼‰
    "ROW_SAMPLE_FRAC": None,

    # æµ‹è¯•é›†æ¯”ä¾‹
    "TEST_RATIO": 0.2,

    # æ˜¯å¦ç”»å›¾
    "PLOT": True,

    # æ˜¯å¦åšSHAPè§£é‡Šï¼ˆå¾ˆæ…¢ï¼‰
    "DO_SHAP": False,

    # DNNå‚æ•°
    "DNN_EPOCHS": 200,
    "DNN_BATCH": 1024,

    # éšæœºç§å­
    "SEED": 42
}


# ---------------------------
# å‰©ä½™ä»£ç ï¼ˆä¸å˜ï¼‰
# ---------------------------



# ---------------------------
# 1. å·¥å…·å‡½æ•°
# ---------------------------

def ensure_outdir(path):
    os.makedirs(path, exist_ok=True)

def parse_datetime(series):
    """æŠŠå¤šç§å¸¸è§å­—æ®µç»Ÿä¸€æˆ pandas datetime (ç§’çº§)ã€‚"""
    # è‡ªåŠ¨è¯†åˆ«/å…¼å®¹ 2022-01-01 09:35:00 / 2022/01/01 09:35 ç­‰
    return pd.to_datetime(series, errors="coerce", utc=False)

def reduce_mem(df):
    """ç®€å•é™å†…å­˜ï¼šæ•´æ•°/æµ®ç‚¹é™ä½å®½ï¼›å­—ç¬¦ä¸²è½¬categoryã€‚"""
    for col in df.columns:
        if pd.api.types.is_integer_dtype(df[col]):
            df[col] = pd.to_numeric(df[col], downcast="integer")
        elif pd.api.types.is_float_dtype(df[col]):
            df[col] = pd.to_numeric(df[col], downcast="float")
        elif pd.api.types.is_object_dtype(df[col]) and df[col].nunique() / max(len(df[col]),1) < 0.5:
            df[col] = df[col].astype("category")
    return df

def cross_section_zscore(g: pd.Series):
    """æ¨ªæˆªé¢ Z-scoreï¼ˆåŒä¸€æ—¶é—´æˆªé¢å†…å¯¹æ‰€æœ‰è‚¡ç¥¨ï¼‰"""
    mean = g.mean()
    std = g.std(ddof=0)
    if std == 0 or np.isnan(std):
        return (g - mean) * 0.0
    return (g - mean) / std

# ---------------------------
# 2. æ•°æ®è¯»å–ä¸åˆå¹¶
# ---------------------------

def load_stock_panel(stock_dir: str,
                     sample_frac: float | None = None) -> pd.DataFrame:
    """
    æ‰¹é‡è¯»å– 5min ä¸ªè‚¡ CSVï¼Œç»Ÿä¸€å­—æ®µï¼š
      code, datetime, open, high, low, close, volume, amount
    """
    files = glob.glob(os.path.join(stock_dir, "*.csv"))
    if len(files) == 0:
        raise FileNotFoundError(f"No CSV files found in: {stock_dir}")

    frames = []
    for f in files:
        try:
            df = pd.read_csv(f)
            # å­—æ®µåå…¼å®¹ï¼šdate/datetime, turnover/amount
            cols = {c.lower(): c for c in df.columns}
            # å¿…è¦åˆ—æ˜ å°„
            mapping = {
                cols.get("code", "code"): "code",
                cols.get("date", cols.get("datetime", "datetime")): "datetime",
                cols.get("open", "open"): "open",
                cols.get("high", "high"): "high",
                cols.get("low", "low"): "low",
                cols.get("close", "close"): "close",
                cols.get("volume", "volume"): "volume",
                cols.get("turnover", cols.get("amount", "turnover")): "amount",
            }
            df = df.rename(columns=mapping)[list(mapping.values())]
            # to numeric
            for c in ["open", "high", "low", "close", "volume", "amount"]:
                df[c] = pd.to_numeric(df[c], errors="coerce")
            df["datetime"] = parse_datetime(df["datetime"])
            df["code"] = df["code"].astype(str)
            if sample_frac is not None:
                df = df.sample(frac=sample_frac, random_state=CONFIG["SEED"])
            frames.append(df)
        except Exception as e:
            print(f"[WARN] Skip file {f}: {e}")

    panel = pd.concat(frames, axis=0, ignore_index=True)
    panel = panel.dropna(subset=["datetime", "code"]).sort_values(["code", "datetime"])
    panel = reduce_mem(panel)
    return panel


def load_index_5min(index_file: str) -> pd.DataFrame:
    """è¯»å–æŒ‡æ•° 5min æ•°æ®ï¼Œç»Ÿä¸€å­—æ®µï¼šdatetime, index_close"""
    if not os.path.exists(index_file):
        raise FileNotFoundError(index_file)
    idx = pd.read_csv(index_file)
    # å­—æ®µåå…¼å®¹
    lower = {c.lower(): c for c in idx.columns}
    idx = idx.rename(columns={
        lower.get("datetime", lower.get("date", "datetime")): "datetime",
        lower.get("index_close", lower.get("close", "index_close")): "index_close"
    })
    idx["datetime"] = parse_datetime(idx["datetime"])
    idx["index_close"] = pd.to_numeric(idx["index_close"], errors="coerce")
    idx = idx.dropna(subset=["datetime", "index_close"]).sort_values("datetime")
    idx = reduce_mem(idx)
    return idx


def load_fundamentals(fund_file: str) -> pd.DataFrame | None:
    """è¯»å–è¡Œä¸š&åŸºæœ¬é¢ï¼ˆå¯é€‰ï¼‰ã€‚æ”¯æŒ xlsx/csvã€‚"""
    if not os.path.exists(fund_file):
        return None
    if fund_file.lower().endswith(".xlsx"):
        fund = pd.read_excel(fund_file)
    else:
        fund = pd.read_csv(fund_file)

    lower = {c.lower(): c for c in fund.columns}
    # è‡³å°‘åŒ…æ‹¬ code, date(or period), industryï¼Œå¯é€‰è´¢åŠ¡å­—æ®µ
    fund = fund.rename(columns={
        lower.get("code", "code"): "code",
        lower.get("date", lower.get("period", "date")): "date",
        lower.get("industry", "industry"): "industry",
        lower.get("market_cap", "market_cap"): "market_cap",
        lower.get("pe_ttm", "pe_ttm"): "pe_ttm",
        lower.get("pb", "pb"): "pb",
        lower.get("roe", "roe"): "roe",
        lower.get("total_asset", "total_asset"): "total_asset",
    })
    fund["code"] = fund["code"].astype(str)
    fund["date"] = pd.to_datetime(fund["date"], errors="coerce")
    for c in ["market_cap", "pe_ttm", "pb", "roe", "total_asset"]:
        if c in fund.columns:
            fund[c] = pd.to_numeric(fund[c], errors="coerce")
    return fund


# ---------------------------
# 3. ç‰¹å¾å·¥ç¨‹ä¸ç›®æ ‡å˜é‡
# ---------------------------

def build_features(panel: pd.DataFrame) -> pd.DataFrame:
    """
    åœ¨ä¸ªè‚¡ 5min é¢æ¿ä¸ŠåŸºäº OHLCV æ„é€ æŠ€æœ¯ç‰¹å¾ï¼ˆæ»šåŠ¨çª—å£æ³¨æ„ä»¥ code åˆ†ç»„ï¼‰
    è¿”å›ï¼šåŠ å…¥ç‰¹å¾åˆ—å’Œä¸ªè‚¡æ”¶ç›Š r_i,t
    """
    df = panel.sort_values(["code", "datetime"]).copy()

    # ä¸ªè‚¡ 5min å¯¹æ•°æ”¶ç›Š
    df["prev_close"] = df.groupby("code")["close"].shift(1)
    df["ret_i"] = np.log(df["close"] / df["prev_close"])

    # vwap è¿‘ä¼¼ï¼ˆé‡‘é¢/é‡ï¼‰
    df["vwap"] = df["amount"] / df["volume"].replace(0, np.nan)

    # çª—å£ï¼ˆå¯æŒ‰éœ€æ‰©å±•ï¼‰
    def g_rolling(s, win, func):
        return s.rolling(win, min_periods=win).apply(func, raw=False)

    # åŠ¨é‡ç±»
    df["ret_5"] = df.groupby("code")["close"].apply(lambda x: np.log(x / x.shift(5)))
    df["ret_10"] = df.groupby("code")["close"].apply(lambda x: np.log(x / x.shift(10)))
    df["reversal_1"] = -df.groupby("code")["ret_i"].shift(1)

    # æ³¢åŠ¨/åŒºé—´ç±»
    df["vol_5"] = df.groupby("code")["ret_i"].rolling(5).std().reset_index(level=0, drop=True)
    df["vol_10"] = df.groupby("code")["ret_i"].rolling(10).std().reset_index(level=0, drop=True)
    df["hl_range"] = (df["high"] - df["low"]) / df["prev_close"]

    # é‡ä»·
    df["volume_ratio"] = df.groupby("code")["volume"].apply(lambda x: x / x.rolling(5).mean())
    df["oc_ret"] = (df["close"] - df["open"]) / df["open"]
    df["ma5"] = df.groupby("code")["close"].rolling(5).mean().reset_index(level=0, drop=True)
    df["ma10"] = df.groupby("code")["close"].rolling(10).mean().reset_index(level=0, drop=True)
    df["ma_bias10"] = (df["close"] - df["ma10"]) / df["ma10"]

    # K çº¿å½¢æ€
    df["upper_shadow"] = df["high"] - np.maximum(df["open"], df["close"])
    df["lower_shadow"] = np.minimum(df["open"], df["close"]) - df["low"]
    body = (df["close"] - df["open"]).abs()
    true_range = (df["high"] - df["low"]).replace(0, np.nan)
    df["body_range_ratio"] = body / true_range

    # é€‰å–è¾“å‡ºçš„å› å­åˆ—
    factor_cols = [
        "ret_5", "ret_10", "reversal_1",
        "vol_5", "vol_10", "hl_range",
        "volume_ratio", "oc_ret",
        "ma_bias10", "upper_shadow", "lower_shadow", "body_range_ratio"
    ]
    return df, factor_cols


def merge_index_and_target(df: pd.DataFrame, index_df: pd.DataFrame) -> pd.DataFrame:
    """
    åˆå¹¶æŒ‡æ•°æ”¶ç›Šå¹¶è®¡ç®—ç›®æ ‡å˜é‡ï¼ˆä¸‹ä¸€æœŸ 5min è¶…é¢æ”¶ç›Šï¼‰
    y_{t+1} = (r_i,t+1 - r_mkt,t+1)
    """
    idx = index_df.copy()
    idx = idx.sort_values("datetime")
    idx["idx_prev"] = idx["index_close"].shift(1)
    idx["ret_mkt"] = np.log(idx["index_close"] / idx["idx_prev"])

    m = pd.merge(df, idx[["datetime", "ret_mkt"]], on="datetime", how="left")
    # ç›®æ ‡å˜é‡ç”¨ t+1ï¼šé¿å…æœªæ¥ä¿¡æ¯æ³„éœ²
    m["ret_i_t1"] = m.groupby("code")["ret_i"].shift(-1)
    m["ret_mkt_t1"] = m["ret_mkt"].shift(-1)
    m["excess_ret_t1"] = m["ret_i_t1"] - m["ret_mkt_t1"]
    return m


def add_cross_section_zscores(df: pd.DataFrame, factor_cols: list[str]) -> pd.DataFrame:
    """å¯¹æ¯ä¸ªæ—¶é—´æˆªé¢çš„æ‰€æœ‰è‚¡ç¥¨åšæ¨ªæˆªé¢ Z-score æ ‡å‡†åŒ–"""
    for col in factor_cols:
        df[col + "_z"] = df.groupby("datetime")[col].transform(cross_section_zscore)
    return df


def merge_fundamentals(df: pd.DataFrame, fund: pd.DataFrame | None) -> pd.DataFrame:
    """æŠŠè¡Œä¸šä¸åŸºæœ¬é¢åˆå¹¶åˆ° 5min é¢æ¿ï¼ˆæŒ‰æœ€è¿‘å¯å¾—å¯¹é½ï¼šasof mergeï¼‰ã€‚"""
    if fund is None:
        return df

    fund = fund.sort_values(["code", "date"])
    # asof éœ€è¦é”®åˆå¹¶ï¼ˆæ¯ä¸ª code ç‹¬ç«‹ï¼‰
    out = []
    for code, g in df.groupby("code", sort=False):
        gg = g.sort_values("datetime")
        ff = fund[fund["code"] == code].sort_values("date")
        if len(ff) == 0:
            out.append(gg)
            continue
        # è¿‘ä¼¼â€œç”¨å½“æ—¶ç‚¹ä¹‹å‰æœ€è¿‘ä¸€æœŸçš„è´¢åŠ¡/è¡Œä¸šâ€
        merged = pd.merge_asof(
            gg, ff, left_on="datetime", right_on="date",
            direction="backward", allow_exact_matches=True
        )
        out.append(merged.drop(columns=["date"], errors="ignore"))
    res = pd.concat(out, axis=0, ignore_index=True)
    return res

# ---------------------------
# 4. å¯è§†åŒ–ï¼ˆä¿å­˜ PNGï¼‰
# ---------------------------

def plot_distribution(y: pd.Series, out_dir: str, title="Excess Return (t+1) Distribution"):
    if not CONFIG["PLOT"]:
        return
    plt.figure(figsize=(8,5))
    if SEABORN:
        sns.histplot(y.dropna(), bins=120, kde=True)
    else:
        plt.hist(y.dropna(), bins=120, alpha=0.85)
    plt.title(title)
    plt.xlabel("Excess Return (t+1)")
    plt.ylabel("Count")
    plt.tight_layout()
    plt.savefig(os.path.join(out_dir, "fig1_excess_return_dist.png"), dpi=300)
    plt.close()

def plot_index_vol(index_df: pd.DataFrame, out_dir: str):
    if not CONFIG["PLOT"]:
        return
    roll = index_df.copy().sort_values("datetime")
    roll["idx_prev"] = roll["index_close"].shift(1)
    roll["ret_mkt"] = np.log(roll["index_close"] / roll["idx_prev"])
    roll["vol_30"] = roll["ret_mkt"].rolling(30).std()

    plt.figure(figsize=(10,4))
    plt.plot(roll["datetime"], roll["vol_30"])
    plt.title("HS300 5-min Rolling Volatility (30 bars)")
    plt.xlabel("Datetime"); plt.ylabel("Vol")
    plt.tight_layout()
    plt.savefig(os.path.join(out_dir, "fig2_index_rolling_vol.png"), dpi=300)
    plt.close()

def plot_xgb_importance(model, feature_names, out_dir: str):
    if not CONFIG["PLOT"]:
        return
    importances = getattr(model, "feature_importances_", None)
    if importances is None:
        return
    imp = pd.Series(importances, index=feature_names).sort_values()
    plt.figure(figsize=(8,6))
    imp.plot(kind="barh")
    plt.title("XGBoost Feature Importance")
    plt.tight_layout()
    plt.savefig(os.path.join(out_dir, "fig3_xgb_importance.png"), dpi=300)
    plt.close()

def plot_corr_heatmap(df: pd.DataFrame, feature_names: list[str], out_dir: str):
    if not CONFIG["PLOT"]:
        return
    # ä¸ºäº†å†…å­˜å’Œé€Ÿåº¦ï¼Œä»…æŠ½æ ·ç”»çƒ­åŠ›å›¾
    samp = df[feature_names].dropna().sample(min(50000, df[feature_names].dropna().shape[0]),
                                            random_state=CONFIG["SEED"])
    corr = samp.corr()
    plt.figure(figsize=(8,6))
    if SEABORN:
        sns.heatmap(corr, cmap="coolwarm", center=0, square=True)
    else:
        plt.imshow(corr, cmap="coolwarm"); plt.colorbar()
        plt.xticks(range(len(feature_names)), feature_names, rotation=90)
        plt.yticks(range(len(feature_names)), feature_names)
    plt.title("Correlation Heatmap (sample)")
    plt.tight_layout()
    plt.savefig(os.path.join(out_dir, "fig4_corr_heatmap.png"), dpi=300)
    plt.close()

def plot_dnn_history(history, out_dir: str):
    if not CONFIG["PLOT"]:
        return
    plt.figure(figsize=(8,4))
    plt.plot(history.history["loss"], label="train")
    if "val_loss" in history.history:
        plt.plot(history.history["val_loss"], label="val")
    plt.title("DNN Learning Curve (MSE)")
    plt.xlabel("Epoch"); plt.ylabel("Loss")
    plt.legend(); plt.tight_layout()
    plt.savefig(os.path.join(out_dir, "fig5_dnn_learning_curve.png"), dpi=300)
    plt.close()

# ---------------------------
# 5. æœºå™¨å­¦ä¹ å»ºæ¨¡
# ---------------------------

def time_split(df: pd.DataFrame, test_ratio=0.2):
    """æŒ‰æ—¶é—´åˆ‡åˆ†è®­ç»ƒ/æµ‹è¯•ï¼Œé¿å…æ³„éœ²"""
    cutoff = df["datetime"].quantile(1 - test_ratio)
    train = df[df["datetime"] < cutoff].copy()
    test = df[df["datetime"] >= cutoff].copy()
    return train, test, cutoff

def run_lasso(X_train, y_train, X_test, y_test, out_dir: str):
    from sklearn.linear_model import LassoCV
    from sklearn.metrics import r2_score, mean_squared_error

    model = LassoCV(cv=5, random_state=CONFIG["SEED"], n_jobs=None)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    r2 = r2_score(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)

    coef = pd.Series(model.coef_, index=X_train.columns)
    coef_nonzero = coef[coef != 0].sort_values(key=np.abs, ascending=False)
    coef_nonzero.to_csv(os.path.join(out_dir, "lasso_nonzero_coef.csv"))
    print(f"[Lasso] R2={r2:.6f}  MSE={mse:.6e}")
    return {"r2": r2, "mse": mse, "model": model}

def run_xgb(X_train, y_train, X_test, y_test, out_dir: str):
    from sklearn.metrics import r2_score, mean_squared_error
    import xgboost as xgb

    model = xgb.XGBRegressor(
        n_estimators=400,
        max_depth=5,
        learning_rate=0.05,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=CONFIG["SEED"],
        n_jobs=-1,
        reg_lambda=1.0
    )
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    r2 = r2_score(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    print(f"[XGBoost] R2={r2:.6f}  MSE={mse:.6e}")

    plot_xgb_importance(model, list(X_train.columns), out_dir)

    # å¯é€‰ï¼šSHAP
    if CONFIG["DO_SHAP"]:
        try:
            import shap
            explainer = shap.Explainer(model)
            # æŠ½æ · 2 ä¸‡åšè§£é‡Šï¼Œé¿å…è¿‡æ…¢
            idx = np.random.RandomState(CONFIG["SEED"]).choice(
                len(X_train), size=min(20000, len(X_train)), replace=False
            )
            shap_values = explainer(X_train.iloc[idx])
            shap.summary_plot(shap_values, X_train.iloc[idx], show=False)
            plt.tight_layout()
            plt.savefig(os.path.join(out_dir, "fig6_shap_summary.png"), dpi=300)
            plt.close()
        except Exception as e:
            print(f"[SHAP] skip: {e}")

    return {"r2": r2, "mse": mse, "model": model}

def run_dnn(X_train, y_train, X_test, y_test, out_dir: str):
    from sklearn.metrics import r2_score, mean_squared_error
    import tensorflow as tf
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.regularizers import l2
    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

    tf.random.set_seed(CONFIG["SEED"])

    model = Sequential([
        Dense(256, input_dim=X_train.shape[1], activation="relu", kernel_regularizer=l2(1e-4)),
        BatchNormalization(), Dropout(0.3),
        Dense(128, activation="relu", kernel_regularizer=l2(1e-4)),
        BatchNormalization(), Dropout(0.3),
        Dense(64, activation="relu", kernel_regularizer=l2(1e-4)),
        Dropout(0.2),
        Dense(1)  # å›å½’
    ])
    model.compile(optimizer=Adam(learning_rate=5e-4), loss="mse")

    es = EarlyStopping(monitor="val_loss", patience=10, restore_best_weights=True)
    rlrop = ReduceLROnPlateau(monitor="val_loss", factor=0.5, patience=5, min_lr=1e-6)

    history = model.fit(
        X_train, y_train,
        epochs=CONFIG["DNN_EPOCHS"],
        batch_size=CONFIG["DNN_BATCH"],
        validation_split=0.2,
        callbacks=[es, rlrop],
        verbose=1
    )
    y_pred = model.predict(X_test, batch_size=CONFIG["DNN_BATCH"]).flatten()
    r2 = r2_score(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    print(f"[DNN] R2={r2:.6f}  MSE={mse:.6e}")

    # å­¦ä¹ æ›²çº¿
    plot_dnn_history(history, out_dir)
    return {"r2": r2, "mse": mse, "model": model}

# ---------------------------
# 6. ä¸»æµç¨‹
# ---------------------------

def main():
    ensure_outdir(CONFIG["OUT_DIR"])
    print(">> Loading stock 5-min panel ...")
    panel = load_stock_panel(CONFIG["STOCK_DIR"], CONFIG["ROW_SAMPLE_FRAC"])

    if CONFIG["ROW_SAMPLE_FRAC"] is not None:
        print(f"âš ï¸ å½“å‰å¯ç”¨äº†æŠ½æ ·ï¼Œä½¿ç”¨æ¯”ä¾‹ = {CONFIG['ROW_SAMPLE_FRAC']}")

    print("ğŸŸ¢ åŸå§‹é¢æ¿æ•°æ®æ£€æŸ¥:")
    print("æ€»è¡Œæ•°ï¼š", len(panel))
    print("è‚¡ç¥¨æ•°é‡ï¼š", panel["code"].nunique())
    print("æ—¶é—´èŒƒå›´ï¼š", panel["datetime"].min(), "~", panel["datetime"].max())
    print("å¹³å‡æ¯åªè‚¡ç¥¨æ•°æ®é‡ï¼š", len(panel) // panel["code"].nunique())

    print(f"   stocks rows = {len(panel):,}, stocks = {panel['code'].nunique()} "
          f"range = [{panel['datetime'].min()} ~ {panel['datetime'].max()}]")

    print(">> Loading HS300 index 5-min ...")
    idx = load_index_5min(CONFIG["INDEX_FILE"])
    print(f"   index rows  = {len(idx):,} range = [{idx['datetime'].min()} ~ {idx['datetime'].max()}]")

    # å¯é€‰åŸºæœ¬é¢/è¡Œä¸š
    fund = load_fundamentals(CONFIG["FUND_FILE"])
    if fund is None:
        print(">> Fundamentals not provided (industry/financials skipped).")
    else:
        print(f">> Fundamentals loaded: rows={len(fund):,}, codes={fund['code'].nunique()}")

    # ç‰¹å¾å·¥ç¨‹
    print(">> Building features ...")
    df, factor_cols = build_features(panel)
    del panel; gc.collect()

    # åˆå¹¶æŒ‡æ•°å¹¶æ„é€ ç›®æ ‡å˜é‡
    print(">> Merging index & building target (t+1 excess return) ...")
    df = merge_index_and_target(df, idx)

    # åˆå¹¶åŸºæœ¬é¢/è¡Œä¸š
    if fund is not None:
        print(">> Merging fundamentals (asof) ...")
        df = merge_fundamentals(df, fund)

    # æ¨ªæˆªé¢ Z-scoreï¼ˆåªå¯¹æŠ€æœ¯å› å­åšï¼‰
    print(">> Cross-section z-scoring ...")
    df = add_cross_section_zscores(df, factor_cols)
    factor_cols_z = [c + "_z" for c in factor_cols]
    print("ğŸŸ¡ ç‰¹å¾å·¥ç¨‹å®Œæˆåæ•°æ®é‡ï¼š", len(df))



       # é€‰æ‹©è®­ç»ƒæ•°æ®åˆ—
    use_cols = factor_cols_z  # å¦‚éœ€æŠŠåŸºæœ¬é¢åŠ å…¥ï¼Œå¯ append åŸºæœ¬é¢å­—æ®µåï¼ˆæ³¨æ„åšæ ‡å‡†åŒ–æˆ–ç¼©æ”¾ï¼‰

    # âœ… è°ƒè¯•ï¼šå…ˆæ‰“å°åˆ—åç¡®è®¤ code å­˜åœ¨
    print("âœ” å½“å‰ df åˆ—åï¼š", df.columns.tolist())

    # âœ… é˜²æ­¢åˆ—ç¼ºå¤±å´©æºƒï¼ˆæ›´ç¨³å¥ï¼‰
    expected_cols = ["datetime", "code", "excess_ret_t1"] + use_cols
    data = df[[col for col in expected_cols if col in df.columns]].dropna()
    print("ğŸ”µ å®é™…ç”¨äºå»ºæ¨¡çš„æ•°æ®é‡ï¼š", len(data))
    print("ä½¿ç”¨ç‡ï¼š{:.2%}".format(len(data) / len(df)))


    # é™å†…å­˜
    data = reduce_mem(data)

    # ä¿å­˜ä¸€äº›æè¿°å›¾
    plot_distribution(data["excess_ret_t1"], CONFIG["OUT_DIR"])
    plot_corr_heatmap(data, use_cols, CONFIG["OUT_DIR"])
    plot_index_vol(idx, CONFIG["OUT_DIR"])

    # æ—¶é—´åˆ‡åˆ†
    train, test, cutoff = time_split(data, CONFIG["TEST_RATIO"])
    print(f">> Train before {cutoff}, rows={len(train):,} | Test after {cutoff}, rows={len(test):,}")

    X_train = train[use_cols]; y_train = train["excess_ret_t1"]
    X_test = test[use_cols];   y_test  = test["excess_ret_t1"]

    print("ğŸŸ£ æ¨¡å‹è¾“å…¥ç»´åº¦æ£€æŸ¥ï¼š")
    print("X_train shape:", X_train.shape)
    print("X_test shape:", X_test.shape)

    # ---- Lasso
    res_lasso = run_lasso(X_train, y_train, X_test, y_test, CONFIG["OUT_DIR"])

    # ---- XGBoost
    res_xgb = run_xgb(X_train, y_train, X_test, y_test, CONFIG["OUT_DIR"])

    # ---- DNN
    res_dnn = run_dnn(X_train, y_train, X_test, y_test, CONFIG["OUT_DIR"])

    # ç»“æœæ±‡æ€»
    summary = {
        "lasso": {"R2": float(res_lasso["r2"]), "MSE": float(res_lasso["mse"])},
        "xgboost": {"R2": float(res_xgb["r2"]), "MSE": float(res_xgb["mse"])},
        "dnn": {"R2": float(res_dnn["r2"]), "MSE": float(res_dnn["mse"])},
        "cutoff": str(cutoff),
        "n_train": int(len(train)),
        "n_test": int(len(test)),
        "n_codes": int(data["code"].nunique()) if "code" in data.columns else 0
    }
    with open(os.path.join(CONFIG["OUT_DIR"], "metrics_summary.json"), "w", encoding="utf-8") as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)

    print("\n===== Summary =====")
    print(json.dumps(summary, indent=2, ensure_ascii=False))
    print(f"\nAll outputs saved to: {CONFIG['OUT_DIR']}")
    # æ”¾åˆ° main() æœ€åæ‰§è¡Œ
    plot_model_comparison(
        summary_json_path=os.path.join(CONFIG["OUT_DIR"], "metrics_summary.json"),
        out_dir=CONFIG["OUT_DIR"]
    )


# ---------------------------
# 7. é™„åŠ å›¾è¡¨ï¼šæ¨¡å‹æ€§èƒ½å¯¹æ¯”
# ---------------------------

def plot_model_comparison(summary_json_path, out_dir):
    import matplotlib.pyplot as plt
    with open(summary_json_path, "r", encoding="utf-8") as f:
        summary = json.load(f)

    models = ["lasso", "xgboost", "dnn"]
    r2_scores = [summary[m]["R2"] for m in models]
    mse_scores = [summary[m]["MSE"] for m in models]

    # RÂ² å¯¹æ¯”å›¾
    plt.figure(figsize=(6, 4))
    plt.bar(models, r2_scores, color='steelblue')
    plt.title("Model Comparison: RÂ²")
    plt.ylabel("RÂ²")
    plt.tight_layout()
    plt.savefig(os.path.join(out_dir, "fig6_model_r2_comparison.png"), dpi=300)
    plt.close()

    # MSE å¯¹æ¯”å›¾
    plt.figure(figsize=(6, 4))
    plt.bar(models, mse_scores, color='indianred')
    plt.title("Model Comparison: MSE")
    plt.ylabel("MSE")
    plt.tight_layout()
    plt.savefig(os.path.join(out_dir, "fig7_model_mse_comparison.png"), dpi=300)
    plt.close()


if __name__ == "__main__":
    main()
